{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4b740e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/nhamlv-55/CFI_NNs/blob/master/MNIST_toy/Experiment-Feb28-xujie-Copy1.ipynb\n",
    "\n",
    "from models import FeedforwardNeuralNetModel, TinyCNN, PatternClassifier, NewTinyCNN\n",
    "import regularizer_losts as rl\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import utils as CFI_utils\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "#from frozendict import frozendict\n",
    "from datetime import datetime\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from absl import app, flags\n",
    "from easydict import EasyDict\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "colors = sns.color_palette(\"tab10\")\n",
    "\n",
    "#Logging stuffs\n",
    "import logging\n",
    "import sys\n",
    "# Create logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Create STDERR handler\n",
    "handler = logging.StreamHandler(sys.stderr)\n",
    "# ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# Create formatter and add it to the handler\n",
    "formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "\n",
    "# Set STDERR handler as the only handler \n",
    "logger.handlers = [handler]\n",
    "\n",
    "#configs\n",
    "epochs = 10\n",
    "batch_size = 1000\n",
    "test_batch_size = 10000\n",
    "stable_batch_size = 60000\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)\n",
    "\n",
    "lr = 0.01\n",
    "log_interval = 100\n",
    "\n",
    "#torch specific configs\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "train_kwargs = {'batch_size': batch_size}\n",
    "\n",
    "test_kwargs = {'batch_size': test_batch_size}\n",
    "stable_kwargs = {'batch_size': stable_batch_size}\n",
    "\n",
    "if use_cuda:\n",
    "    cuda_kwargs = {'num_workers': 1,\n",
    "                   'pin_memory': True,\n",
    "                   'shuffle': True}\n",
    "    train_kwargs.update(cuda_kwargs)\n",
    "    test_kwargs.update(cuda_kwargs)\n",
    "    stable_kwargs.update(cuda_kwargs)\n",
    "class Shift:\n",
    "    def __init__(self, shift = 0):\n",
    "        print(\"alive\")\n",
    "        self.shift = shift\n",
    "\n",
    "    def __call__(self, arr):\n",
    "        print(\"running\")\n",
    "        #print(arr)\n",
    "        return arr\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"{self.__class__.__name__}()\"\n",
    "    \n",
    "class Rescale(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, landmarks = sample['image'], sample['landmarks']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = transform.resize(image, (new_h, new_w))\n",
    "\n",
    "        # h and w are swapped for landmarks because for images,\n",
    "        # x and y axes are axis 1 and 0 respectively\n",
    "        landmarks = landmarks * [new_w / w, new_h / h]\n",
    "\n",
    "        return {'image': img, 'landmarks': landmarks}\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "   \n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "   \n",
    "  \n",
    "])\n",
    "\n",
    "def shift_and_roll(arr : torch.Tensor, x : int , y : int) -> torch.Tensor:\n",
    "    return torch.roll(arr, shifts = (x,y), dims = (0,1)) #indexing\n",
    "def shift(arr : torch.Tensor, x : int , y : int) -> torch.Tensor:\n",
    "\n",
    "    pad = [0,0,0,0]\n",
    "    x_start = 0\n",
    "    x_end = 0\n",
    "    y_start = 0\n",
    "    y_end = 0\n",
    "    if x >= 0:\n",
    "        pad[0] = x\n",
    "        x_start = 0\n",
    "        x_end = 28\n",
    "    else:\n",
    "        pad[1] = abs(x)\n",
    "        x_start = -28\n",
    "         \n",
    "         \n",
    "        \n",
    "         \n",
    "    \n",
    "    if y >= 0:\n",
    "        pad[2] = y\n",
    "        y_start = 0\n",
    "        y_end = 28\n",
    "    else:\n",
    "        pad[3] = abs(y)\n",
    "        y_start= -28\n",
    "        print(\"ys is {}\".format(y_start))\n",
    "    \n",
    "\n",
    "    padder = torch.nn.ZeroPad2d(tuple(pad))\n",
    "  \n",
    "    result = padder(arr)\n",
    "\n",
    "    if y < 0:\n",
    "        y_end = result.shape[0]\n",
    "    if x < 0:\n",
    "        x_end = result.shape[1]\n",
    "    \n",
    "    return result[y_start:y_end, x_start:x_end]\n",
    "\n",
    "def noisify(arr : torch.Tensor , distribution : torch.distributions.Distribution) -> torch.Tensor: #randomly add noise\n",
    "   \n",
    "    #print(distribution.sample(arr.size()).shape\\\\)\n",
    "    #print(torch.reshape(distribution.sample(arr.size()), (28,28)).shape)\n",
    "   \n",
    "    noise = torch.reshape(distribution.sample(arr.size()), (arr.shape))\n",
    "    return arr + noise\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "   \n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0efc2efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_dataset(dataset):\n",
    "    '''\n",
    "    DO A STATE CHANGE\n",
    "    '''\n",
    "    shift_x = 1\n",
    "    shift_y = 0\n",
    "    mu = 1\n",
    "    sigma = 1\n",
    "    do_shift = False\n",
    "    do_noisfy = True\n",
    "    modification_string = \"base\"\n",
    "    if do_shift:\n",
    "        modification_string += \" -shift {} {} - \".format(shift_x, shift_y)\n",
    "    if do_noisfy:\n",
    "        modification_string += \" -noise added using gaussian using mean {} and stddev {}- \".format(mu, sigma)\n",
    "    for i in range(dataset.data.shape[0]):\n",
    "        if do_shift:\n",
    "            dataset.data[i,:,:] = shift(dataset.data[i,:,:],shift_x, shift_y)\n",
    "        \n",
    "        if do_noisfy:\n",
    "            gaussian = torch.distributions.Normal(loc = mu, scale = sigma)# loc = mu, scale = stddev\n",
    "\n",
    "            dataset.data[i,:,:] = noisify(dataset.data[i,:,:], gaussian)\n",
    "    return modification_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21a0a8",
   "metadata": {},
   "source": [
    "# Train/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff123fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -accuracy 0.9042-\n"
     ]
    }
   ],
   "source": [
    "def check_gradient(grad, label, last_sorted_grads, plot = False):\n",
    "    logging.info(\"CHECKING GRADIENT FOR LABEL {}\".format(label))\n",
    "    sum_abs_grad = np.sum(abs(grad[label]), axis = 0)\n",
    "    \n",
    "    current_sorted_grad = (-sum_abs_grad).argsort()\n",
    "    \n",
    "#     if len(last_sorted_grads[label]) > 0:\n",
    "#         for k in [100, 200, 300, 400]:\n",
    "#             prev_top_k = set(last_sorted_grads[label][-1][:k])\n",
    "#             current_top_k = set(current_sorted_grad[:k])\n",
    "#             intersect = prev_top_k.intersection(current_top_k)\n",
    "#             logging.info('k = {}. How many top Gradients are stable since last epoch?: {}'.format(k, len(intersect)))\n",
    "        \n",
    "    for k in [0, 9, 99, 199]:    \n",
    "        logging.debug('{}th biggest gradient = {}'.format(k, np.sort(-sum_abs_grad)[k]))\n",
    "    if plot:\n",
    "        fig = plt.figure(figsize=(30, 1))\n",
    "        plt.bar(range(sum_abs_grad.shape[0]), sum_abs_grad)\n",
    "        plt.show()\n",
    "        print(sum_abs_grad.max(), sum_abs_grad.argmax(), sum_abs_grad.min())\n",
    "\n",
    "    return current_sorted_grad\n",
    "\n",
    "\n",
    "\n",
    "#init stuffs\n",
    "LOAD = True\n",
    "# LOADPATH = 'FFN19-17-24'\n",
    "LOADPATH = 'FFN18_28_21'\n",
    "# LOADPATH = 'TinyCNN14-28-29'\n",
    "\n",
    "LAST_N_EPOCHS = 10\n",
    "\n",
    "dataset1 = datasets.MNIST('./data', train=True, download=False,\n",
    "                          transform=transform)\n",
    "dataset2 = datasets.MNIST('./data', train=False, download=False,\n",
    "                          transform=transform)\n",
    "\n",
    "\n",
    "#transform_dataset(dataset1)\n",
    "#modification_string = transform_dataset(dataset2)\n",
    "modification_string = \"\"\n",
    "train_loader = torch.utils.data.DataLoader(dataset1, **train_kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
    "\n",
    "model = FeedforwardNeuralNetModel(28*28, 128, 10).to(device)\n",
    "# model = TinyCNN().to(device)\n",
    "\n",
    "if LOAD:\n",
    "    model.load_state_dict(torch.load(LOADPATH))\n",
    "else:\n",
    "\n",
    "    epochs = 10\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=lr)\n",
    "\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma = 0.7)\n",
    "    last_sorted_grads = defaultdict(list)\n",
    "    \n",
    "    all_rows = []\n",
    "    \n",
    "    for epoch in tqdm(range(1, epochs + 1)):\n",
    "        model.register_gradient()\n",
    "        model.train()\n",
    "        target_log  = None # need to record the label to match with the gradient later\n",
    "        for data, target in train_loader:\n",
    "            target_log = np.concatenate((target_log, target), axis = 0) if target_log is not None else target\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.float())\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        CFI_utils.test(model, device, test_loader)\n",
    "#         scheduler.step()\n",
    "        grad = CFI_utils.get_grad_each_label(model.gradient_log, \n",
    "                                      target_log = target_log, \n",
    "                                      layers = ['fc1', 'fc2', 'fc3', 'fc4'], \n",
    "#                                       layers = ['conv1', 'conv2','fc1', 'fc2'],\n",
    "                                      labels = range(10))\n",
    "        torch.save(model.state_dict(), model.model_savename())\n",
    "        \n",
    "        row_data = []\n",
    "        for label in range(10):\n",
    "            r = []\n",
    "            logging.info(\"After {} epoch:\".format(epoch))\n",
    "            last_sorted_grads[label].append(check_gradient(grad, label, last_sorted_grads))\n",
    "            \n",
    "            \n",
    "            if epoch >= LAST_N_EPOCHS:\n",
    "                for k in [100, 200, 300, 400]:\n",
    "                    all_top_k = [set(sorted_grad[:k]) for sorted_grad in last_sorted_grads[label][-LAST_N_EPOCHS:]]\n",
    "                    intersect = set.intersection(*all_top_k)\n",
    "                    logging.info('k = {}. How many top Gradients are stable among all last {} epochs?: {}'.format(k, LAST_N_EPOCHS, len(intersect)))\n",
    "\n",
    "                    \n",
    "\n",
    "with torch.no_grad():\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for data, target in dataset2:\n",
    "        data_extend = data[None, :]\n",
    "#         if int(torch.argmax(model.cpu()(data.cpu()), dim = 1)) == target:\n",
    "        if int(torch.argmax(model.cuda()(data_extend.cuda()), dim = 1)) == target:\n",
    "            correct += 1\n",
    "        total+=1\n",
    "    \"accuracy {}\".format( correct/total)\n",
    "    modification_string += \" -accuracy {}-\".format( correct/total)\n",
    "    print(modification_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917debe8",
   "metadata": {},
   "source": [
    "#### Compute all patterns in the training set, and put them into corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a683b28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "root - INFO - (5923, 458)\n",
      "root - INFO - (6742, 458)\n",
      "root - INFO - (5958, 458)\n",
      "root - INFO - (6131, 458)\n",
      "root - INFO - (5842, 458)\n",
      "root - INFO - (5421, 458)\n",
      "root - INFO - (5918, 458)\n",
      "root - INFO - (6265, 458)\n",
      "root - INFO - (5851, 458)\n",
      "root - INFO - (5949, 458)\n",
      "root - INFO - (980, 458)\n",
      "root - INFO - (1135, 458)\n",
      "root - INFO - (1032, 458)\n",
      "root - INFO - (1010, 458)\n",
      "root - INFO - (982, 458)\n",
      "root - INFO - (892, 458)\n",
      "root - INFO - (958, 458)\n",
      "root - INFO - (1028, 458)\n",
      "root - INFO - (974, 458)\n",
      "root - INFO - (1009, 458)\n"
     ]
    }
   ],
   "source": [
    "class Patterns:\n",
    "    def __init__(self, model: torch.nn.Module, dataloader: torch.utils.data.DataLoader, labels, layers):\n",
    "        self._model = model\n",
    "        self.label2patterns = {}\n",
    "        self.label2idx = {}\n",
    "        self._labels = labels\n",
    "        self._layers = layers\n",
    "        self._dataloader = dataloader\n",
    "        self._populate()\n",
    "        \n",
    "    def _populate(self):\n",
    "        \n",
    "        label2patterns = {}\n",
    "        label2idx = {}\n",
    "        for label in self._labels:\n",
    "            patterns = []\n",
    "            filter_ids = []\n",
    "            \n",
    "            for data, target in self._dataloader:\n",
    "                \n",
    "                flter = np.where(target == label)\n",
    "                filter_ids.append(flter)\n",
    "                data = data[flter]\n",
    "                logging.debug(data.shape[0])\n",
    "                pattern = self._model.get_pattern(data, layers, device, flatten = True)\n",
    "                logging.debug(pattern.shape)\n",
    "                patterns.append(pattern)\n",
    "\n",
    "            patterns = np.squeeze(np.concatenate(patterns, axis = 0))\n",
    "            filter_ids = np.squeeze(np.concatenate(filter_ids, axis = 0))\n",
    "            label2patterns[label] = patterns\n",
    "            label2idx[label] = filter_ids\n",
    "            \n",
    "            logging.info(patterns.shape)\n",
    "        \n",
    "        #freeze\n",
    "        self.label2patterns = dict(label2patterns)\n",
    "        self.label2idx = dict(label2idx)\n",
    "        \n",
    "    def apply_filter(self, f):\n",
    "        pass\n",
    "    \n",
    "    def unique():\n",
    "        pass\n",
    "    \n",
    "    def query_pattern():\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "layers = ['fc1', 'fc2', 'fc3', 'fc4']\n",
    "# layers = ['conv1', 'conv2','fc1', 'fc2']\n",
    "\n",
    "labels = range(10)\n",
    "K = 25\n",
    "stable_loader = torch.utils.data.DataLoader(dataset1, **stable_kwargs)\n",
    "\n",
    "all_patterns = Patterns(model = model,\n",
    "                        dataloader = stable_loader,\n",
    "                        labels = labels,\n",
    "                        layers = layers)\n",
    "all_test_patterns = Patterns(model = model,\n",
    "                        dataloader = test_loader,\n",
    "                        labels = labels,\n",
    "                        layers = layers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbaf2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can only study gradient if the model is trained\n",
    "# FIXME: should save gradient into a pickle as well\n",
    "if not LOAD:\n",
    "    for K in [25, 50, 100]:\n",
    "        print(\"K=\", K)\n",
    "        row = []\n",
    "        row.append(str(K))\n",
    "        for label in labels:\n",
    "            #construct the stable gradients \n",
    "            all_top_k = [set(sorted_grad[:K]) for sorted_grad in last_sorted_grads[label][-LAST_N_EPOCHS:]]\n",
    "            intersect = set.intersection(*all_top_k)\n",
    "            stable_grad = np.array(sorted(list(intersect)))\n",
    "            print(\"There are {} stable grad in top K\".format(len(stable_grad)))\n",
    "            print(stable_grad)\n",
    "\n",
    "            logging.info(all_patterns[label].shape)\n",
    "            print(\"LABEL:\", label)\n",
    "            print(\"how many unique paths in the full pattern?\", np.unique(all_patterns[label], axis = 0).shape)\n",
    "            print(\"how many unique paths in the filtered pattern?\", np.unique(all_patterns[label][:, stable_grad ], axis = 0).shape)\n",
    "            print(\"how many unique paths in the randomly filtered pattern?\", \n",
    "                  np.unique(patterns[:, \n",
    "                                     np.random.choice(458, len(stable_grad), replace = False) ], axis = 0).shape)\n",
    "            row.append(\"|\".join([str(len(stable_grad)),\n",
    "                                 str(np.unique(all_patterns[label], axis = 0).shape[0]),\n",
    "                                 str(np.unique(all_patterns[label][:, stable_grad ], axis = 0).shape[0]),\n",
    "                                 str(np.unique(all_patterns[label][:, np.random.choice(458, len(stable_grad), replace = False) ], axis = 0).shape[0])\n",
    "                                ]))\n",
    "        all_rows.append(\",\".join(row)+\"\\n\")\n",
    "\n",
    "    with open(\"gradient_exp_log_2.csv\", \"w\") as f:\n",
    "        f.write(\"K,\"+\",\".join([str(l) for l in labels]) + \"\\n\")\n",
    "        f.writelines(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e7f003a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For class 0, there are totally 881 unique patterns for the last hidden layer.\n",
      "The dominant pattern has frequency value 330.\n",
      "\n",
      "For class 1, there are totally 698 unique patterns for the last hidden layer.\n",
      "The dominant pattern has frequency value 971.\n",
      "\n",
      "For class 2, there are totally 2621 unique patterns for the last hidden layer.\n",
      "The dominant pattern has frequency value 114.\n",
      "\n",
      "For class 3, there are totally 1697 unique patterns for the last hidden layer.\n",
      "The dominant pattern has frequency value 134.\n",
      "\n",
      "For class 4, there are totally 2333 unique patterns for the last hidden layer.\n",
      "The dominant pattern has frequency value 145.\n",
      "\n",
      "For class 5, there are totally 1666 unique patterns for the last hidden layer.\n",
      "The dominant pattern has frequency value 132.\n",
      "\n",
      "For class 6, there are totally 2520 unique patterns for the last hidden layer.\n",
      "The dominant pattern has frequency value 100.\n",
      "\n",
      "For class 7, there are totally 1688 unique patterns for the last hidden layer.\n",
      "The dominant pattern has frequency value 315.\n",
      "\n",
      "For class 8, there are totally 867 unique patterns for the last hidden layer.\n",
      "The dominant pattern has frequency value 498.\n",
      "\n",
      "For class 9, there are totally 1230 unique patterns for the last hidden layer.\n",
      "The dominant pattern has frequency value 1063.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# class NpEncoder(json.JSONEncoder):\n",
    "#     def default(self, obj):\n",
    "#         if isinstance(obj, np.integer):\n",
    "#             return int(obj)\n",
    "#         if isinstance(obj, np.floating):\n",
    "#             return float(obj)\n",
    "#         if isinstance(obj, np.bool_):\n",
    "#             return int(obj)\n",
    "#         if isinstance(obj, np.ndarray):\n",
    "#             return obj.tolist()\n",
    "#         return super(NpEncoder, self).default(obj)\n",
    "    \n",
    "\n",
    "    \n",
    "label_to_last_hidden_layer_patterns = {}\n",
    "for label in all_patterns.label2patterns:\n",
    "    last_layer_to_patterns = []\n",
    "    label_patterns = all_patterns.label2patterns[label]\n",
    "    last_hidden_layer_patterns = label_patterns[:, 384:448]\n",
    "    unique_last_hidden_layer_patterns, freq = np.unique(last_hidden_layer_patterns, axis = 0, return_counts=True)\n",
    "    print(f\"For class {label}, there are totally {len(freq)} unique patterns for the last hidden layer.\")\n",
    "    print(f\"The dominant pattern has frequency value {np.max(freq)}.\\n\")\n",
    "    for i in range(len(unique_last_hidden_layer_patterns)):\n",
    "        r = unique_last_hidden_layer_patterns[i]\n",
    "        pattern_idx = (last_hidden_layer_patterns == r).all(axis=1)\n",
    "        last_layer_to_patterns.append((r, label_patterns[pattern_idx]))\n",
    "        assert pattern_idx.sum() == freq[i]\n",
    "    label_to_last_hidden_layer_patterns[label] = last_layer_to_patterns\n",
    "\n",
    "# json.dump(label_to_last_hidden_layer_patterns, open(\"last_layer_patterns_log{}.json\".format(datetime.now().strftime(\"%H-%M-%S\")), \"w\")) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b309cb35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
